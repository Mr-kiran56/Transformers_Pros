{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "image_size = 224\n",
        "hidden_size = 256\n",
        "num_layers = (4, 4)\n",
        "num_heads = 8\n",
        "patch_size = 16\n",
        "learning_rate = 1e-4\n",
        "nepochs = 40\n",
        "batch_size = 32\n",
        "accumulation_steps = 2\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomCrop(image_size, padding=4, padding_mode='reflect'),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.AutoAugment(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.2)\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdOWYTHzCjUr",
        "outputId": "3bfd432a-218c-4e6c-e444-47959dc09837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, img_folder, caption_file, transform=None, mode='train', val_split=0.1):\n",
        "        super().__init__()\n",
        "        self.img_folder = img_folder\n",
        "        self.caption_file = caption_file\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "\n",
        "\n",
        "        with open(self.caption_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "\n",
        "        caption_dict = {}\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(',', 1)\n",
        "            if len(parts) == 2:\n",
        "                file_name, caption = parts\n",
        "                file_name = file_name.strip()\n",
        "                caption = caption.strip()\n",
        "                caption_dict[file_name] = caption\n",
        "\n",
        "\n",
        "        images = [f for f in os.listdir(self.img_folder)\n",
        "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "\n",
        "        self.data = []\n",
        "        missing = 0\n",
        "\n",
        "        for f_name in images:\n",
        "            f_base = os.path.splitext(f_name)[0]\n",
        "\n",
        "            if f_name in caption_dict:\n",
        "                self.data.append((f_name, caption_dict[f_name]))\n",
        "            elif f_base in caption_dict:\n",
        "                self.data.append((f_name, caption_dict[f_base]))\n",
        "            else:\n",
        "                missing += 1\n",
        "\n",
        "        print(f\" Loaded {len(self.data)} image-caption pairs. Missing {missing} captions.\")\n",
        "\n",
        "\n",
        "        np.random.seed(42)\n",
        "        indices = np.random.permutation(len(self.data))\n",
        "        split_idx = int(len(self.data) * (1 - val_split))\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.data = [self.data[i] for i in indices[:split_idx]]\n",
        "        else:\n",
        "            self.data = [self.data[i] for i in indices[split_idx:]]\n",
        "\n",
        "        print(f\" {mode.capitalize()} set: {len(self.data)} images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, caption = self.data[idx]\n",
        "        img_path = os.path.join(self.img_folder, img_name)\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, caption\n",
        "\n",
        "\n",
        "train_dataset = ImageCaptionDataset(\n",
        "    \"/content/drive/MyDrive/images\",\n",
        "    \"/content/drive/MyDrive/captions.txt\",\n",
        "    train_transform,\n",
        "    mode='train'\n",
        ")\n",
        "\n",
        "val_dataset = ImageCaptionDataset(\n",
        "    \"/content/drive/MyDrive/images\",\n",
        "    \"/content/drive/MyDrive/captions.txt\",\n",
        "    val_transform,\n",
        "    mode='val'\n",
        ")\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_data_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Train DataLoader: {len(train_data_loader)} batches\")\n",
        "print(f\"Val DataLoader: {len(val_data_loader)} batches\")\n",
        "\n",
        "test_img, captions = next(iter(train_data_loader))\n",
        "print(f\"Image batch shape: {test_img.shape}\")\n",
        "print(f\"Sample caption: {captions[0]}\")\n"
      ],
      "metadata": {
        "id": "7h1FBLoGGmwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "class TokenDrop(nn.Module):\n",
        "    def __init__(self, prob=0.15, blank_token=1, eos_token=102):\n",
        "        super().__init__()\n",
        "        self.prob = prob\n",
        "        self.eos_token = eos_token\n",
        "        self.blank_token = blank_token\n",
        "\n",
        "    def forward(self, sample):\n",
        "        mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
        "\n",
        "        can_drop = (~(sample == self.eos_token)).long()\n",
        "        mask = mask * can_drop\n",
        "\n",
        "        mask[:, 0] = torch.zeros_like(mask[:, 0]).long()\n",
        "\n",
        "        replace_with = (self.blank_token * torch.ones_like(sample)).long()\n",
        "\n",
        "        sample_out = (1 - mask) * sample + mask * replace_with\n",
        "\n",
        "        return sample_out"
      ],
      "metadata": {
        "id": "tHFUrFeOGp7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_patches(image_tensor, patch_size=16):\n",
        "    bs, c, h, w = image_tensor.size()\n",
        "\n",
        "    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
        "    unfolded = unfold(image_tensor)\n",
        "    unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n",
        "\n",
        "    return unfolded"
      ],
      "metadata": {
        "id": "rGhE7G08GrX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb"
      ],
      "metadata": {
        "id": "-0T4n8eoGtCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_emb, hidden_size=256, num_layers=4, num_heads=8, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
        "\n",
        "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.02)\n",
        "\n",
        "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_size * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "\n",
        "        self.decoder_layers = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
        "\n",
        "    def forward(self, input_seq, encoder_output, input_padding_mask=None, encoder_padding_mask=None):\n",
        "        input_embs = self.embedding(input_seq)\n",
        "        bs, l, h = input_embs.shape\n",
        "\n",
        "        seq_indx = torch.arange(l, device=input_seq.device)\n",
        "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
        "        embs = input_embs + pos_emb\n",
        "        embs = self.dropout(embs)\n",
        "\n",
        "        causal_mask = torch.triu(torch.ones(l, l, device=input_seq.device), 1).bool()\n",
        "\n",
        "        output = self.decoder_layers(\n",
        "            tgt=embs,\n",
        "            memory=encoder_output,\n",
        "            tgt_mask=causal_mask,\n",
        "            tgt_key_padding_mask=input_padding_mask,\n",
        "            memory_key_padding_mask=encoder_padding_mask\n",
        "        )\n",
        "\n",
        "        output = self.layer_norm(output)\n",
        "        return self.fc_out(output)"
      ],
      "metadata": {
        "id": "ptCnrJwxGxiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class VisionEncoder(nn.Module):\n",
        "    def __init__(self, image_size, channels_in, patch_size=16, hidden_size=256, num_layers=4, num_heads=8, dropout=0.1):\n",
        "        super(VisionEncoder, self).__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n",
        "\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc_in.weight)\n",
        "\n",
        "        seq_length = (image_size // patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std=0.02))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_size * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "\n",
        "        self.encoder_layers = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, image):\n",
        "        bs = image.shape[0]\n",
        "\n",
        "        patch_seq = extract_patches(image, patch_size=self.patch_size)\n",
        "        patch_emb = self.fc_in(patch_seq)\n",
        "\n",
        "        embs = patch_emb + self.pos_embedding\n",
        "        embs = self.dropout(embs)\n",
        "\n",
        "        output = self.encoder_layers(embs)\n",
        "        output = self.layer_norm(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "b9EO68PGG2-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VisionEncoderDecoder(nn.Module):\n",
        "    def __init__(self, image_size, channels_in, num_emb, patch_size=16,\n",
        "                 hidden_size=256, num_layers=(4, 4), num_heads=8, dropout=0.1):\n",
        "        super(VisionEncoderDecoder, self).__init__()\n",
        "\n",
        "        self.encoder = VisionEncoder(\n",
        "            image_size=image_size,\n",
        "            channels_in=channels_in,\n",
        "            patch_size=patch_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers[0],\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            num_emb=num_emb,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers[1],\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, input_image, target_seq, padding_mask):\n",
        "        bool_padding_mask = padding_mask == 0\n",
        "\n",
        "        encoded_seq = self.encoder(image=input_image)\n",
        "        decoded_seq = self.decoder(\n",
        "            input_seq=target_seq,\n",
        "            encoder_output=encoded_seq,\n",
        "            input_padding_mask=bool_padding_mask\n",
        "        )\n",
        "        return decoded_seq"
      ],
      "metadata": {
        "id": "E7TE_r_0G41J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "caption_model = VisionEncoderDecoder(\n",
        "    image_size=image_size,\n",
        "    channels_in=test_img.shape[1],\n",
        "    num_emb=tokenizer.vocab_size,\n",
        "    patch_size=patch_size,\n",
        "    num_layers=num_layers,\n",
        "    hidden_size=hidden_size,\n",
        "    num_heads=num_heads,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    caption_model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=0.01,\n",
        "    betas=(0.9, 0.999)\n",
        ")\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=nepochs * len(train_data_loader),\n",
        "    eta_min=learning_rate * 0.1\n",
        ")\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "td = TokenDrop(0.15)"
      ],
      "metadata": {
        "id": "iMPgB8hVHAOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_model_params = sum(p.numel() for p in caption_model.parameters())\n",
        "print(f\"Model Parameters: {num_model_params:,} (~{num_model_params//1e6}M)\")\n",
        "\n",
        "\n",
        "training_loss_logger = []\n",
        "val_loss_logger = []\n",
        "best_val_loss = float('inf')\n",
        "start_epoch = 0\n",
        "\n",
        "checkpoint_path = \"captioning_model_4k.pt\"\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(\"Loading existing checkpoint...\")\n",
        "    cp = torch.load(checkpoint_path, map_location=device)\n",
        "    caption_model.load_state_dict(cp[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(cp[\"optimizer_state_dict\"])\n",
        "    training_loss_logger = cp[\"train_data_logger\"]\n",
        "    val_loss_logger = cp[\"val_data_logger\"]\n",
        "    start_epoch = cp[\"epoch\"]\n",
        "    best_val_loss = cp.get(\"best_val_loss\", float('inf'))\n",
        "    print(f\"Resumed from epoch {start_epoch}\")\n"
      ],
      "metadata": {
        "id": "k-FDriuUHCZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "for epoch in trange(start_epoch, nepochs, leave=True, desc=\"Epochs\"):\n",
        "\n",
        "    caption_model.train()\n",
        "    epoch_train_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for batch_idx, (images, captions) in enumerate(tqdm(train_data_loader, desc=f\"Training Epoch {epoch+1}\", leave=False)):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "\n",
        "        tokens = tokenizer(\n",
        "            captions,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=32,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        token_ids = tokens['input_ids'].to(device)\n",
        "        padding_mask = tokens['attention_mask'].to(device)\n",
        "\n",
        "        target_ids = token_ids[:, 1:].contiguous()\n",
        "        tokens_in = td(token_ids)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            pred = caption_model(images, tokens_in[:, :-1], padding_mask=padding_mask[:, :-1])\n",
        "            loss = loss_fn(pred.transpose(1, 2), target_ids)\n",
        "\n",
        "\n",
        "        loss = loss / accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_train_loss += loss.item() * accumulation_steps\n",
        "        num_batches += 1\n",
        "        training_loss_logger.append(loss.item() * accumulation_steps)\n",
        "\n",
        "    avg_train_loss = epoch_train_loss / num_batches\n",
        "\n",
        "\n",
        "    caption_model.eval()\n",
        "    epoch_val_loss = 0\n",
        "    val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, captions in tqdm(val_data_loader, desc=\"Validation\", leave=False):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "\n",
        "            tokens = tokenizer(\n",
        "                captions,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=32,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            token_ids = tokens['input_ids'].to(device)\n",
        "            padding_mask = tokens['attention_mask'].to(device)\n",
        "\n",
        "            target_ids = token_ids[:, 1:].contiguous()\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                pred = caption_model(images, token_ids[:, :-1], padding_mask=padding_mask[:, :-1])\n",
        "                loss = loss_fn(pred.transpose(1, 2), target_ids)\n",
        "\n",
        "            epoch_val_loss += loss.item()\n",
        "            val_batches += 1\n",
        "\n",
        "    avg_val_loss = epoch_val_loss / val_batches\n",
        "    val_loss_logger.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{nepochs}\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_data_logger': training_loss_logger,\n",
        "            'val_data_logger': val_loss_logger,\n",
        "            'model_state_dict': caption_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_val_loss': best_val_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\" Saved new best model with val loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "ojU9lnz6HHDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(training_loss_logger, alpha=0.7)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_loss_logger, 'r-', linewidth=2)\n",
        "plt.title(\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a_Zy3u5FHLU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_caption(model, image_tensor, tokenizer, max_length=30, temperature=0.7, top_k=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            image_embedding = model.encoder(image_tensor.to(device))\n",
        "\n",
        "\n",
        "        generated = [tokenizer.cls_token_id]\n",
        "\n",
        "        for i in range(max_length):\n",
        "            input_tokens = torch.tensor([generated], device=device)\n",
        "\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model.decoder(input_tokens, image_embedding)\n",
        "\n",
        "\n",
        "            next_token_logits = outputs[0, -1, :] / temperature\n",
        "\n",
        "            if top_k > 0:\n",
        "                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                next_token_logits[indices_to_remove] = -float('inf')\n",
        "\n",
        "            next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(next_token_probs, num_samples=1).item()\n",
        "\n",
        "            generated.append(next_token)\n",
        "\n",
        "\n",
        "            if next_token == tokenizer.sep_token_id:\n",
        "                break\n",
        "\n",
        "        caption = tokenizer.decode(generated, skip_special_tokens=True)\n",
        "        return caption\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING ON VALIDATION IMAGES\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "cKuTzPCWHNrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_indices = [0, 1, 2, 3, 4]\n",
        "\n",
        "for idx in test_indices:\n",
        "    img_path, true_caption = val_dataset.data[idx]\n",
        "\n",
        "    print(f\"\\n Image {idx+1}: {img_path}\")\n",
        "    print(f\"True caption: {true_caption}\")\n",
        "\n",
        "\n",
        "    img = Image.open(f\"{val_dataset.img_folder}/{img_path}\").convert(\"RGB\")\n",
        "    img_tensor = val_transform(img).unsqueeze(0)\n",
        "\n",
        "\n",
        "    generated_caption = generate_caption(caption_model, img_tensor, tokenizer, temperature=0.7)\n",
        "    print(f\"Generated: {generated_caption}\")\n",
        "\n",
        "    true_words = set(true_caption.lower().split())\n",
        "    gen_words = set(generated_caption.lower().split())\n",
        "    common_words = true_words.intersection(gen_words)\n",
        "    similarity = len(common_words) / max(len(true_words), 1)\n",
        "    print(f\" Word overlap: {similarity:.2f}\")\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"True: {true_caption}\\nGenerated: {generated_caption}\", fontsize=10)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n Training and evaluation completed!\")"
      ],
      "metadata": {
        "id": "pnnFjzcIGYok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wUUCm2VhGYlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wi-tvPbzGYi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ym4fDsDXGYhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q5hlMg8UGYej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjfpYCtyqdnm"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# import torchvision.transforms as transforms\n",
        "# import os\n",
        "# from torch import nn\n",
        "# import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kw68zelLHjt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image_size=224"
      ],
      "metadata": {
        "id": "AJAl2HAmxfb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_transform = transforms.Compose([transforms.Resize(image_size),\n",
        "#                                       transforms.RandomCrop(image_size),\n",
        "#                                       transforms.AutoAugment(),\n",
        "#                                       transforms.ToTensor(),\n",
        "#                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                                            std=[0.229, 0.224, 0.225])])"
      ],
      "metadata": {
        "id": "XEsj_-bNwzAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from PIL import Image\n",
        "# from torch.utils.data import Dataset\n",
        "\n",
        "# class ImageCaptionDataset(Dataset):\n",
        "#     def __init__(self, img_folder, caption_file, transform=None):\n",
        "#         super().__init__()\n",
        "#         self.img_folder = img_folder\n",
        "#         self.caption_file = caption_file\n",
        "#         self.transform = transform\n",
        "\n",
        "#         # Load all captions\n",
        "#         with open(self.caption_file, 'r', encoding='utf-8') as f:\n",
        "#             lines = f.readlines()\n",
        "\n",
        "#         # Store captions in a dictionary for quick lookup\n",
        "#         caption_dict = {}\n",
        "#         for line in lines:\n",
        "#             parts = line.strip().split(',', 1)\n",
        "#             if len(parts) == 2:\n",
        "#                 file_name, caption = parts\n",
        "#                 file_name = file_name.strip()\n",
        "#                 caption = caption.strip()\n",
        "#                 caption_dict[file_name] = caption\n",
        "\n",
        "#         # Get all image filenames\n",
        "#         images = [f for f in os.listdir(self.img_folder)\n",
        "#                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "#         self.data = []\n",
        "#         missing = 0\n",
        "\n",
        "#         # Match images with their captions\n",
        "#         for f_name in images:\n",
        "#             # Some CSVs might not include file extensions\n",
        "#             f_base = os.path.splitext(f_name)[0]\n",
        "\n",
        "#             if f_name in caption_dict:\n",
        "#                 self.data.append((f_name, caption_dict[f_name]))\n",
        "#             elif f_base in caption_dict:\n",
        "#                 self.data.append((f_name, caption_dict[f_base]))\n",
        "#             else:\n",
        "#                 missing += 1\n",
        "\n",
        "#         print(f\"✅ Loaded {len(self.data)} image-caption pairs. ❌ Missing {missing} captions.\")\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_name, caption = self.data[idx]\n",
        "#         img_path = os.path.join(self.img_folder, img_name)\n",
        "\n",
        "#         img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "#         if self.transform:\n",
        "#             img = self.transform(img)\n",
        "\n",
        "#         return img, caption\n"
      ],
      "metadata": {
        "id": "w3q70NulqjhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset=ImageCaptionDataset(\"/content/drive/MyDrive/images\",\"/content/drive/MyDrive/captions.txt\",train_transform)"
      ],
      "metadata": {
        "id": "bjS1xqQPqjji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(dataset)"
      ],
      "metadata": {
        "id": "4uhW-PQO5f_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images=[f for f in os.listdir(\"/content/drive/MyDrive/NewdataCaptions/images\")]\n",
        "# print(len(images))"
      ],
      "metadata": {
        "id": "1IeOTsNC5VgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from matplotlib import pyplot as plt\n",
        "# import matplotlib.image as image"
      ],
      "metadata": {
        "id": "NPVRE3io3HVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img_path,caption=dataset.data[140]\n",
        "# print(caption)"
      ],
      "metadata": {
        "id": "3ZCouHhD6Gwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img_to_display = image.imread(f\"{dataset.img_folder}/{img_path}\")\n",
        "# plt.imshow(img_to_display)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Nwn91Qf5qjvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data_loader=DataLoader(dataset,batch_size=16,shuffle=True,num_workers=1)"
      ],
      "metadata": {
        "id": "m8QT1grBqj0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g9HPhXHVb_a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(train_data_loader)"
      ],
      "metadata": {
        "id": "k_3-oKcnqj4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_img,captions=next(iter(train_data_loader))\n",
        "# print(len(test_img),len(captions))\n",
        "# print(test_img.shape[1])"
      ],
      "metadata": {
        "id": "hDo0gG8b7ruQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "yLAd1X2e5C1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# images, captions = next(iter(train_data_loader))\n",
        "\n",
        "# tokens = tokenizer(\n",
        "#     captions,\n",
        "#     padding='max_length',\n",
        "#     truncation=True,\n",
        "#     max_length=32,\n",
        "#     return_tensors='pt'\n",
        "# )\n",
        "\n"
      ],
      "metadata": {
        "id": "cpBmWxMq6V6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Image batch shape:\", images.shape)\n",
        "# print(\"Token IDs shape:\", tokens['input_ids'].shape)\n",
        "# print(\"Sample tokens:\", captions[1])\n",
        "# print(\"Token IDs:\", tokens['input_ids'][1])\n",
        "# print(tokenizer.decode(tokens['input_ids'][1]))"
      ],
      "metadata": {
        "id": "Y2oez9jm6V2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class TokenDrop(nn.Module):\n",
        "\n",
        "#     def __init__(self, prob=0.1, blank_token=1, eos_token=102):\n",
        "#         self.prob = prob\n",
        "#         self.eos_token = eos_token\n",
        "#         self.blank_token = blank_token\n",
        "\n",
        "#     def __call__(self, sample):\n",
        "#         mask = torch.bernoulli(self.prob * torch.ones_like(sample)).long()\n",
        "\n",
        "#         can_drop = (~(sample == self.eos_token)).long()\n",
        "#         mask = mask * can_drop\n",
        "\n",
        "#         mask[:, 0] = torch.zeros_like(mask[:, 0]).long()\n",
        "\n",
        "#         replace_with = (self.blank_token * torch.ones_like(sample)).long()\n",
        "\n",
        "#         sample_out = (1 - mask) * sample + mask * replace_with\n",
        "\n",
        "#         return sample_out"
      ],
      "metadata": {
        "id": "CKgRP6WT6Vzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_patches(image_tensor, patch_size=16):\n",
        "#     bs, c, h, w = image_tensor.size()\n",
        "\n",
        "#     unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "#     unfolded = unfold(image_tensor)\n",
        "\n",
        "#     unfolded = unfolded.transpose(1, 2).reshape(bs, -1, c * patch_size * patch_size)\n",
        "\n",
        "#     return unfolded"
      ],
      "metadata": {
        "id": "NgmQ-gii6Vxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class SinusoidalPosEmb(nn.Module):\n",
        "#     def __init__(self, dim):\n",
        "#         super().__init__()\n",
        "#         self.dim = dim\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         device = x.device\n",
        "#         half_dim = self.dim // 2\n",
        "#         emb = math.log(10000) / (half_dim - 1)\n",
        "#         emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "#         emb = x[:, None] * emb[None, :]\n",
        "#         emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "#         return emb\n",
        "\n"
      ],
      "metadata": {
        "id": "X-H04nNz6VwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
        "#         super(Decoder, self).__init__()\n",
        "\n",
        "#         self.embedding = nn.Embedding(num_emb, hidden_size)\n",
        "\n",
        "#         self.embedding.weight.data = 0.001 * self.embedding.weight.data\n",
        "\n",
        "\n",
        "#         self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
        "\n",
        "#         decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads,\n",
        "#                                                    dim_feedforward=hidden_size * 4, dropout=0.0,\n",
        "#                                                    batch_first=True)\n",
        "\n",
        "#         self.decoder_layers = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "\n",
        "#         self.fc_out = nn.Linear(hidden_size, num_emb)\n",
        "\n",
        "#     def forward(self, input_seq, encoder_output, input_padding_mask=None,\n",
        "#                 encoder_padding_mask=None):\n",
        "\n",
        "#         input_embs = self.embedding(input_seq)\n",
        "#         bs, l, h = input_embs.shape\n",
        "\n",
        "\n",
        "#         seq_indx = torch.arange(l, device=input_seq.device)\n",
        "#         pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
        "#         embs = input_embs + pos_emb\n",
        "#         causal_mask = torch.triu(torch.ones(l, l, device=input_seq.device), 1).bool()\n",
        "\n",
        "#         output = self.decoder_layers(tgt=embs, memory=encoder_output, tgt_mask=causal_mask,\n",
        "#                                      tgt_key_padding_mask=input_padding_mask,\n",
        "#                                      memory_key_padding_mask=encoder_padding_mask)\n",
        "\n",
        "#         return self.fc_out(output)\n"
      ],
      "metadata": {
        "id": "bkPllhH26VuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# class VisionEncoder(nn.Module):\n",
        "#     def __init__(self, image_size, channels_in, patch_size=16, hidden_size=128, num_layers=3, num_heads=4):\n",
        "#         super(VisionEncoder, self).__init__()\n",
        "\n",
        "#         self.patch_size = patch_size\n",
        "#         self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n",
        "\n",
        "#         seq_length = (image_size // patch_size) ** 2\n",
        "#         self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std=0.02))\n",
        "\n",
        "\n",
        "#         encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads,\n",
        "#                                                    dim_feedforward=hidden_size * 4, dropout=0.0,\n",
        "#                                                    batch_first=True)\n",
        "\n",
        "#         self.encoder_layers = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "#     def forward(self, image):\n",
        "#         bs = image.shape[0]\n",
        "\n",
        "#         patch_seq = extract_patches(image, patch_size=self.patch_size)\n",
        "#         patch_emb = self.fc_in(patch_seq)\n",
        "\n",
        "\n",
        "#         embs = patch_emb + self.pos_embedding\n",
        "\n",
        "\n",
        "#         output = self.encoder_layers(embs)\n",
        "\n",
        "#         return output\n"
      ],
      "metadata": {
        "id": "dwzTDOng6Vrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# class VisionEncoderDecoder(nn.Module):\n",
        "#     def __init__(self, image_size, channels_in, num_emb, patch_size=16,\n",
        "#                  hidden_size=128, num_layers=(3, 3), num_heads=4):\n",
        "#         super(VisionEncoderDecoder, self).__init__()\n",
        "\n",
        "\n",
        "#         self.encoder = VisionEncoder(image_size=image_size, channels_in=channels_in, patch_size=patch_size,\n",
        "#                                hidden_size=hidden_size, num_layers=num_layers[0], num_heads=num_heads)\n",
        "\n",
        "#         self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size,\n",
        "#                                num_layers=num_layers[1], num_heads=num_heads)\n",
        "\n",
        "#     def forward(self, input_image, target_seq, padding_mask):\n",
        "\n",
        "#         bool_padding_mask = padding_mask == 0\n",
        "\n",
        "\n",
        "#         encoded_seq = self.encoder(image=input_image)\n",
        "\n",
        "#         decoded_seq = self.decoder(input_seq=target_seq,\n",
        "#                                    encoder_output=encoded_seq,\n",
        "#                                    input_padding_mask=bool_padding_mask)\n",
        "#         return decoded_seq"
      ],
      "metadata": {
        "id": "Drk9ga7i6Vpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch import optim\n",
        "# device = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# hidden_size = 192\n",
        "\n",
        "# num_layers = (6, 6)\n",
        "\n",
        "# num_heads = 8\n",
        "\n",
        "# patch_size = 8\n",
        "# learning_rate=1e-4\n",
        "# image_size=225\n",
        "# nepochs=28\n",
        "\n",
        "\n",
        "# caption_model = VisionEncoderDecoder(image_size=image_size, channels_in=test_img.shape[1],\n",
        "#                                      num_emb=tokenizer.vocab_size, patch_size=patch_size,\n",
        "#                                      num_layers=num_layers,hidden_size=hidden_size,\n",
        "#                                      num_heads=num_heads).to(device)\n",
        "\n",
        "# optimizer = optim.Adam(caption_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "# loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "\n",
        "# td = TokenDrop(0.36)\n",
        "\n",
        "\n",
        "# training_loss_logger = []\n",
        "# eval_loss_logger = []\n",
        "# start_epoch = 0"
      ],
      "metadata": {
        "id": "CWrNvas66Vnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# num_model_params = 0\n",
        "# for param in caption_model.parameters():\n",
        "#     num_model_params += param.flatten().shape[0]\n",
        "\n",
        "# print(\"-This Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
      ],
      "metadata": {
        "id": "UXTZhSYg6VlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# print(os.listdir())\n"
      ],
      "metadata": {
        "id": "6fjNDUm9Exuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import math\n",
        "# from tqdm.notebook import trange,tqdm\n",
        "# for epoch in trange(start_epoch, nepochs, leave=False, desc=\"Epoch\"):\n",
        "#     caption_model.train()\n",
        "#     for images, captions in tqdm(train_data_loader, desc=\"Training\", leave=False):\n",
        "\n",
        "#         images = images.to(device)\n",
        "\n",
        "#         tokens = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "#         token_ids = tokens['input_ids'].to(device)\n",
        "#         padding_mask = tokens['attention_mask'].to(device)\n",
        "#         bs = token_ids.shape[0]\n",
        "\n",
        "#         target_ids = torch.cat((token_ids[:, 1:],\n",
        "#                                 torch.zeros(bs, 1, device=device).long()), 1)\n",
        "\n",
        "#         tokens_in = td(token_ids)\n",
        "#         with torch.cuda.amp.autocast():\n",
        "\n",
        "#             pred = caption_model(images, tokens_in, padding_mask=padding_mask)\n",
        "\n",
        "\n",
        "#         loss_mask = (~(target_ids == 0)).float()\n",
        "#         loss = (loss_fn(pred.transpose(1, 2), target_ids) * loss_mask).sum()/loss_mask.sum()\n",
        "\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         scaler.scale(loss).backward()\n",
        "#         scaler.step(optimizer)\n",
        "#         scaler.update()\n",
        "\n",
        "#         training_loss_logger.append(loss.item())\n",
        "\n",
        "\n",
        "#     caption_model.eval()\n",
        "#     with torch.no_grad():\n",
        "\n",
        "#         for i, (images, captions) in enumerate(tqdm(train_data_loader, desc=\"Eval\", leave=False)):\n",
        "#           if i==50:\n",
        "#             break\n",
        "#           else:\n",
        "\n",
        "#             images = images.to(device)\n",
        "\n",
        "\n",
        "#             tokens = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "#             token_ids = tokens['input_ids'].to(device)\n",
        "#             padding_mask = tokens['attention_mask'].to(device)\n",
        "#             bs = token_ids.shape[0]\n",
        "\n",
        "\n",
        "#             target_ids = torch.cat((token_ids[:, 1:],\n",
        "#                                     torch.zeros(bs, 1, device=device).long()), 1)\n",
        "\n",
        "#             with torch.cuda.amp.autocast():\n",
        "\n",
        "#                 pred = caption_model(images, token_ids, padding_mask=padding_mask)\n",
        "\n",
        "\n",
        "#             loss_mask = (~(target_ids == 0)).float()\n",
        "#             loss = (loss_fn(pred.transpose(1, 2), target_ids) * loss_mask).sum()/loss_mask.sum()\n",
        "\n",
        "\n",
        "#             eval_loss_logger.append(loss.item())\n",
        "\n",
        "#     torch.save({'epoch': epoch + 1,\n",
        "#                 'train_data_logger': training_loss_logger,\n",
        "#                 'eval_data_logger': eval_loss_logger,\n",
        "#                 'model_state_dict': caption_model.state_dict(),\n",
        "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                  }, \"captioning_model.pt\")"
      ],
      "metadata": {
        "id": "8Iaf0ShH6VgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cp = torch.load(\"captioning_model.pt\", map_location=\"cpu\")\n",
        "\n",
        "# caption_model.load_state_dict(cp[\"model_state_dict\"])\n",
        "# optimizer.load_state_dict(cp[\"optimizer_state_dict\"])\n",
        "# training_loss_logger = cp[\"train_data_logger\"]\n",
        "# eval_loss_logger = cp[\"eval_data_logger\"]\n",
        "# start_epoch = cp[\"epoch\"]"
      ],
      "metadata": {
        "id": "l8mbsSs_FKLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# _ = plt.figure(figsize=(10, 5))\n",
        "# _ = plt.plot(training_loss_logger[1000:])\n",
        "# _ = plt.title(\"Training Loss\")"
      ],
      "metadata": {
        "id": "NAoIpQzM6VYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #%matplotlib inline\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# window_size = 512\n",
        "# plt.figure(figsize=(10, 5))\n",
        "\n",
        "# train_array = np.array(training_loss_logger)\n",
        "# eval_array = np.array(eval_loss_logger)\n",
        "\n",
        "# print(\"Train logger length:\", len(train_array))\n",
        "# print(\"Eval logger length:\", len(eval_array))\n",
        "\n",
        "# if len(train_array) > window_size and len(eval_array) > window_size:\n",
        "#     train_data = np.convolve(train_array, np.ones(window_size)/window_size, mode=\"valid\")\n",
        "#     eval_data = np.convolve(eval_array, np.ones(window_size)/window_size, mode=\"valid\")\n",
        "\n",
        "#     plt.plot(np.linspace(0, nepochs, len(train_data)), train_data, label=\"Train Loss\")\n",
        "#     plt.plot(np.linspace(0, nepochs, len(eval_data)), eval_data, label=\"Eval Loss\")\n",
        "#     plt.title(\"Train/Eval Loss\")\n",
        "#     plt.xlabel(\"Epochs\")\n",
        "#     plt.ylabel(\"Loss\")\n",
        "#     plt.legend()\n",
        "#     plt.show()\n",
        "# else:\n",
        "#     print(\" Not enough data points for smoothing — try smaller window_size.\")\n"
      ],
      "metadata": {
        "id": "n9YpybEP6VVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# dataiter = next(iter(train_data_loader))\n",
        "# test_images, test_captions = dataiter"
      ],
      "metadata": {
        "id": "sIl7iuVQCmfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img_path,caption=dataset.data[600]\n",
        "# index =3\n",
        "# test_image = img_path # Reverted to img_path\n",
        "# # test_image = img_tensor # Commenting out this line"
      ],
      "metadata": {
        "id": "ASnr0oVOCmcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torchvision\n",
        "# from PIL import Image\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.figure(figsize = (3,3))\n",
        "\n",
        "# # Load the image from the file path\n",
        "# img = Image.open(f\"{dataset.img_folder}/{test_image}\").convert(\"RGB\")\n",
        "\n",
        "# # Apply the same transformations as the training data\n",
        "# if dataset.transform:\n",
        "#     img_tensor = dataset.transform(img)\n",
        "# else:\n",
        "#     img_tensor = transforms.ToTensor()(img)\n",
        "\n",
        "# # Add a batch dimension to the tensor\n",
        "# img_tensor = img_tensor.unsqueeze(0)\n",
        "\n",
        "# out = torchvision.utils.make_grid(img_tensor, 1, normalize=True)\n",
        "# _ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
        "# # The variable test_captions is a tuple of captions for a batch of images.\n",
        "# # The variable `index` was used previously to select a caption from a batch, but\n",
        "# # the current test_image is a single image loaded from a file path.\n",
        "# # The caption for this specific image (img_path) is stored in the `caption` variable\n",
        "# # in cell ASnr0oVOCmcD.\n",
        "# print(caption)"
      ],
      "metadata": {
        "id": "uCQrB2LkCmZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Add the Start-Of-Sentence token to the prompt to signal the network to start generating the caption\n",
        "# sos_token = 101 * torch.ones(1, 1).long()\n",
        "\n",
        "\n",
        "# # Set the temperature for sampling during generation\n",
        "# temp = 0.5"
      ],
      "metadata": {
        "id": "PkkcwvamCmXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn.functional as F\n",
        "# from torch.distributions import Categorical"
      ],
      "metadata": {
        "id": "Z7XXArW8N7Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log_tokens = [sos_token]\n",
        "# caption_model.eval()\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     # Encode the input image\n",
        "#     with torch.cuda.amp.autocast():\n",
        "#         # Forward pass\n",
        "#         image_embedding = caption_model.encoder(img_tensor.to(device)) # Changed from images to test_image\n",
        "\n",
        "#     # Generate the answer tokens\n",
        "#     for i in range(30):\n",
        "#         input_tokens = torch.cat(log_tokens, 1)\n",
        "\n",
        "#         # Decode the input tokens into the next predicted tokens\n",
        "#         data_pred = caption_model.decoder(input_tokens.to(device), image_embedding)\n",
        "\n",
        "#         # Sample from the distribution of predicted probabilities\n",
        "#         next_tokens = torch.argmax(data_pred[:, -1], dim=-1).reshape(1, 1)\n",
        "\n",
        "\n",
        "#         # Append the next predicted token to the sequence\n",
        "#         log_tokens.append(next_tokens.cpu())\n",
        "\n",
        "#         # Break the loop if the End-Of-Caption token is predicted\n",
        "#         if next_tokens.item() == 102:\n",
        "#             break"
      ],
      "metadata": {
        "id": "ub8l32JbNUzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert the list of token indices to a tensor\n",
        "# pred_text = torch.cat(log_tokens, 1)\n",
        "\n",
        "# # Convert the token indices to their corresponding strings using the vocabulary\n",
        "# pred_text = tokenizer.decode(pred_text[0], skip_special_tokens=True)\n",
        "# print(pred_text)\n",
        "\n",
        "\n",
        "# # # Join the token strings to form the predicted text\n",
        "# # pred_text = \"\".join(pred_text_strings)"
      ],
      "metadata": {
        "id": "O9VfKztsNUwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Lets visualise an entire batch of images!\n",
        "# plt.figure(figsize = (3, 3))\n",
        "# out = torchvision.utils.make_grid(test_image, 1, normalize=True)\n",
        "# _ = plt.imshow(out.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "# # Print the predicted text\n",
        "# print(pred_text)"
      ],
      "metadata": {
        "id": "fSTljPi1NUuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKIsUZWfNUrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XN85dPtLNUpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RtboOF3SNUmb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}